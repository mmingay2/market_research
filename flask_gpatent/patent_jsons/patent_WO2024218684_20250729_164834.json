{
  "patent_id": "WO2024218684",
  "url": "https://patents.google.com/patent/WO2024218684",
  "scraped_at": "2025-07-29T16:48:34.112927",
  "title": "WO2024218684A1 - Optical dispersive element for use with neuromorphic camera in a laser warning systems (lws)",
  "publication_number": "US202363460252P",
  "authority": "United States",
  "legal_status": null,
  "application_number": null,
  "inventors": [
    "Antony Orth",
    "contemplate the various aspects of the technology in any number of claim forms For example",
    "reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the technology"
  ],
  "assignees": [
    "National Research Council Of Canada",
    "Title"
  ],
  "priority_date": null,
  "filing_date": "2024-04-17",
  "publication_date": "2024-10-24",
  "grant_date": null,
  "expiration_date": null,
  "abstract": "A target located method and apparatus for the detection and wavelength identification of lasers directed at the target using neuromorphic cameras.",
  "claims": [
    "Claims\n\n1 . An optical dispersive element for use with a neuromorphic camera Laser Warning System (LWS) comprising: a modified transparent or reflective surface such that an incoming laser produces a diffraction pattern on the sensor of the camera. \n\n\n2. The optical dispersive element of claim 1 , wherein the modified transparent or reflective surface comprises a phase grating or an amplitude grating. \n\n\n3. The optical dispersive element of claim 2, wherein the phase grating or the amplitude grating comprises a pattern selected from: a plurality of slits; a grating; a 2D grid; or a combination thereof. \n\n\n4. The optical dispersive element of claim 3, wherein the pattern occurs at a density of between 100 sections/mm and 1500 sections/mm. \n\n\n5. The optical dispersive element of any one of claims 1 to 4, wherein the optical dispersive element is selected from: a holographic film; a holographic grating on a rigid substrate, an etched glass relief grating, or a polymer replicated grating. \n\n\n6. A method of detecting the wavelength of a laser with a neuromorphic camera Laser Warning System (LWS) comprising: placing an optical dispersive element comprising a modified transparent or reflective surface at a distance from the lens so that an incoming laser produces a diffraction pattern on the sensor of the camera; and detecting the spacing between the peaks of the diffraction pattern to determine the wavelength of the incoming laser. \n\n\n\n\n7. The method of detecting the wavelength of a laser of claim 6, wherein the diffraction pattern comprises either 2 or 3 peaks. \n\n\n8. The method of detecting the wavelength of a laser of claim 7 , further comprising determining which of the peaks is the 0th order peak and using the location of the 0th order peak on the camera sensor to determine the angle of incidence of the incoming laser. \n\n\n9. The method of detecting the wavelength of any one of claims 6 to 8, wherein the modified transparent or reflective surface comprises a phase grating or an amplitude grating. \n\n\n10. The method of detecting the wavelength of a laser of claim 9, wherein the phase grating or the amplitude grating comprises a pattern selected from: a plurality of slits; a grating; a 2D grid; or a combination thereof. \n\n\n11 .The method of detecting the wavelength of a laser of claim 10, wherein the pattern occurs at a density of between 100 sections/mm and 1500 sections/mm. \n\n\n12. The method of detecting the wavelength of a laser of any one of claims 6 to 11 , wherein the optical dispersive element is selected from: a holographic film, a holographic grating on a rigid substrate, an etched glass relief grating, or a polymer replicated grating."
  ],
  "description": "Optical Dispersive Element for Use With Neuromorphic Camera in a Laser Warning Systems (LWS) \nFIELD OF THE INVENTION \nLaser warning systems and more specifically laser warning systems incorporating neuromorphic cameras with an optical dispersive element configured for identifying the wavelength of the laser being detected. \nBACKGROUND \nLasers aimed at aircraft put the safety of pilots, crews and passengers at risk. During a laser incident, the pilot may become distracted or temporarily blinded during critical maneuvers. \nThe use of a laser detector, known as a laser warning system (LWS), can confirm the moment of the incident, the origin of the laser pointer and the wavelength, intensity, and pulsing frequency and pulse duration. This information can be used for prosecution and for evaluating the risk of eye injury. \nLWSs are also widely used in military applications for threat detection. Camera-based LWSs have higher angular resolution than photodiode-based systems due to the larger number of pixels. However, photodiode-based systems are smaller and draw less power. Although a small footprint LWS is desirable, the physical size of the aperture can limit the sensitivity of the overall system. These tradeoffs must be considered in choosing the appropriate LWS for a given application. \nGenerally, LWSs should have wide field-of-view in order to limit the number of systems to implement on a platform for having a full coverage against potential incidence of laser threats. In addition to its field-of-view, LWSs must have high angular resolution to provide precise information on the laser beam origin. The possibility for the LWS to measure the repetition rate, pulse duration, intensity, and the wavelength of incident laser beams would also allow a better identification of the laser threats and the capability to provide the best protection. Finally, in parallel to these requirements, LWSs \n\nmust not trigger on bright events like, for examples, glittering of sunlight on water surface or from light reflections from street signs to avoid false positive alarms. \nSUMMARY \nAccording to an aspect of the invention there is herein described in greater detail an optical dispersive element for use with a neuromorphic camera Laser Warning System (LWS) comprising: a modified transparent or reflective surface such that an incoming laser produces a diffraction pattern on the sensor of the camera. \nVariants of this aspect include: The optical dispersive element wherein the modified transparent or reflective surface comprises a phase grating or an amplitude grating; The optical dispersive element wherein the phase grating or the amplitude grating comprises a pattern selected from: a plurality of slits, a grating, a 2D grid, or a combination thereof; The optical dispersive element wherein the pattern occurs at a density of between 100 sections/mm and 1500 sections/mm; The optical dispersive element wherein the optical dispersing element is selected from a holographic film, a holographic grating on a rigid substrate, an etched glass relief grating, or a polymer replicated grating. \nAccording to another aspect of the invention there is herein described in greater detail a method of detecting the wavelength of a laser with a neuromorphic camera Laser Warning System (LWS) comprising: an optical dispersive element comprising a modified transparent or reflective surface at a distance from the lens so that an incoming laser produces a diffraction pattern on the sensor of the camera; and detecting the spacing between the peaks of the diffraction pattern to determine the wavelength of the incoming laser. \nVariants of this other aspect include: The method wherein the diffraction pattern comprises either 2 or 3 peaks; The method further comprising determining which of the peaks is the Oth order peak and using the location of the Oth order peak on the camera sensor to determine the angle of incidence of the incoming laser; The method wherein \n\nthe modified transparent or reflective surface comprises a phase grating or an amplitude grating; The method wherein the phase grating or the amplitude grating comprises a pattern selected from:: a plurality of slits; a grating; a 2D grid; or a combination thereof; The method wherein the pattern occurs at a density of between 100 sections/mm and 1500 sections/mm; The method wherein the optical dispersive element is selected from: a holographic film, holographic grating on a rigid substrate, an etched glass relief grating, or a polymer replicated grating. \nBRIEF DESCRIPTION OF THE DRAWINGS \nThe drawings have not necessarily been drawn to scale. Similarly, some components and/or operations can be separated into different blocks or combined into a single block for the purposes of discussion of some of the implementations of the present technology. Moreover, while the technology is amenable to various modifications and alternative forms, specific implementations have been shown by way of example in the drawings and are described in detail below. The intention, however, is not to limit the technology to the particular implementations described. On the contrary, the technology is intended to cover all modifications, equivalents, and alternatives falling within the scope of the technology as defined by the appended claims. \nFigure 1 is a schematic setup and event density output according to an aspect of the invention. \nFigure 2 is a polynomial fit according to an aspect of the invention. \nFigure 3 is a x-y distribution of statistical results according to an aspect of the invention.\nFigure 4 is a graph of angle error according to an aspect of the invention. \nFigure 5 a graph of event frequency response function and frequency cutoff according to an aspect of the invention. \nFigure 6 is a series of photographs showing the diffraction pattern generated by an incident laser passing through a 1000 lines/mm holographic film grating when the laser is entering the detector at ~0 degrees, ~20 degrees, and ~45 degrees. \nDETAILED DESCRIPTION \n\nSystems and methods for Laser detection are described herein. In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of implementations of the present technology. It will be apparent, however, to one skilled in the art that implementations of the present technology can be practiced without some of these specific details. \nThe techniques introduced here can be implemented as special-purpose hardware (for example, circuitry), as programmable circuitry appropriately programmed with software and/or firmware, or as a combination of special-purpose and programmable circuitry. Hence, implementations can include a machine-readable medium having stored thereon instructions which can be used to program a computer (or other electronic devices) to perform a process. The machine-readable medium can include, but is not limited to, floppy diskettes, optical disks, compact disc read-only memories (CD-ROMs), magnetooptical disks, ROMs, random access memories (RAMs), erasable programmable readonly memories (EPROMs), electrically erasable programmable read-only memories (EEPROMs), magnetic or optical cards, flash memory, or other type of media/machine- readable medium suitable for storing electronic instructions. \nThe phrases “in some implementations,” “according to some implementations,” “in the implementations shown,” “in other implementations,” and the like generally mean the particular feature, structure, or characteristic following the phrase is included in at least one implementation of the present technology, and can be included in more than one implementation. In addition, such phrases do not necessarily refer to the same implementations or different implementations. \nUnlike traditional cameras that record the intensity of light and produce an image of the target, a neuromorphic camera records variation in the light intensity in time. This can be thought of as a differential or first order derivative of the intensity. These changes in light-intensity, or events, are why neuromorphic cameras are commonly called event cameras. \nNeuromorphic cameras are attractive for use in a LWS because laser attacks are expected to be infrequent events and neuromorphic cameras require processing only \n\nwhen events are registered. This would enable a LWS to be deployed with limited power consumption and a small physical footprint compared to a LWS based on a traditional image sensor. Moreover, for fast moving laser threats, localization with a neuromorphic sensor is not restricted by the frame rate of a synchronous readout camera, which also requires a significant power draw to operate at an elevated refresh-rates. A neuromorphic LWS has the potential to combine the best of both worlds - high resolution, high sensitivity laser threat detection with a low power draw. \nAccording to an aspect of the invention there is provided a Laser Warning System (LWS) comprising a neuromorphic camera, and a fisheye lens, wherein the fisheye lens is coupled to the neuromorphic camera along an optical path in slight defocus. \nIn order to verify the efficacy of this approach, an experimental test bench was built to assess the performance of a neuromorphic camera for laser event detection and localization. A schematic of the system is shown in Fig. 1 . The camera (an iniVation DAVIS346) was mounted on an automated rotation stage (PI M-060PD) in the path of a collimated laser beam (658nm). The laser beam was collimated from the output of a single mode fiber by a 2-inch diameter 200mm focal length plano-convex lens. The camera was fitted with a fisheye lens (Edmund Optics 62-274) that filled the camera sensor with a circle of diameter approximately equal to the frame height. Thus, the camera observes a full hemisphere field of view (FOV). \nThe image projected onto the camera sensor is made slightly out of focus by using a c- mount spacer ring between the lens and camera. Because of this defocus, the image of the collimated laser beam on the camera sensor was approximately 10 pixels in diameter when the fisheye lens aperture is set to f/4. This defocus increased the precision in localizing a light source incident on the lens. If the laser beam was instead imaged in focus, it would have spanned less than a pixel on the sensor. In this situation, localization precision is poor due to the relatively large discretization and low fill factor of the sensor. However, when imaged with defocus, the event-weighted centroid of the ~10 pixels wide spot was reliably estimated to within a fraction of a pixel diameter. The improvement in estimation due to defocus could also be achieved \n\nthrough placement of an optical dispersive element in the optical path. This could be in conjunction with a spacer or in the alternative. \nBecause the collimated laser beam was located at optical infinity, the displacement of the image of a focused spot is expected to vary linearly on the image sensor under the paraxial (small angle) approximation. \nThe spot position vs. angle relationship was measured experimentally by acquiring 4s of event data for 10 equally spaced stage rotation angles from -90 to + 90 degrees. The laser beam was set to pulse at 10Hz with a duty cycle of 5%. The time averaged power incident on the fisheye lens was 81 nW (all powers reported are for the total power incident on the 50mm diameter fisheye lens). From this 4s event stream for each angle, the event-weighted centroid was calculated. To filter out noise, a morphological opening with a 3x3 square pixel kernel was performed prior to centroid calculation. \nIn Fig. 2a we show the event-weighted centroid position along the x-axis (the direction of rotation) as a function of stage angle. Although the trend is nearly linear, a linear fit fails to accurately capture the position of the spot at the extremes of the FOV (Fig. 2b). The root-mean-squared error (RMSE) of the linear fit is .024 degrees when averaged over the 180 degrees range of rotation. Although the error of a linear fit is large at the edges of the FOV, the slope of the fit gives an approximate indication of the angular sampling of the camera and fisheye system: 0.58 degrees per pixel. To eliminate the systematic errors from a linear calibration, we instead fit a 5th order polynomial to the data. This polynomial fit yielded a RMSE of 0.013 degrees with roughly uniform magnitude over the FOV. \nWe assessed the resolution of the neuromorphic laser warning system by measuring the standard deviation of a stationary spot. The laser was set to pulse at 10Hz with 5% duty cycle, and for each pulse the event-weighted centroid was calculated. A typical distribution of the spot centroids (shown in blue) is shown in Fig. 3a for an angle of incidence of - 30 degrees and aperture setting of f/4; the mean position of the spot centroids is shown in orange. The same measurement was performed for an angle of incidence ranging from -90 to 0 degrees along the horizontal (x) direction. The resulting \n\nmean angle from the average centroid is shown in Fig. 3b. It was found that f/4 provides a good tradeoff between spot size and increased signal, with repeatability degrading at either very small or very large aperture settings. \nThe accuracy of the neuromorphic LWS was investigated by measuring laser spot positions at varying stage angles across the FOV and comparing with the ground truth angle of incidence given by the stage position. After finding the centroid position on the image sensor, the measured angle of incidence of the laser was found according to the polynomial fit. A typical example of the difference between the measured angle and the stage angle is shown in Fig. 4. For this example, the laser was modulated at 80Hz (duty cycle 50%), with an integration time of 1 s. The RMSE of the measured angle of incidence in this case is .054 degrees. \nThis RMSE value depends on the laser modulation frequency due to the high pass filter in the neuromorphic camera circuitry. For all experimental data herein, the bias settings were tuned manually to increase responsiveness at high frequencies. In Fig. 5a, (same illumination parameters as for Fig. 4), the RMSE (green dashed curve) initially improves with increasing laser modulation frequency (due to more events per unit time) and then degrades rapidly at ~1 kHz when the cutoff frequency of the neuromorphic camera's hardware is reached. To illustrate the effect of frequency on the response of the neuromorphic camera, we measured the event frequency response function for incident powers ranging from 20nW to 9631 nW, as shown in Fig. 5a. While the event response rate drops sharply at higher frequencies, the value of the cut off frequency increases with increasing power. We define the cutoff frequency as the frequency at which the number of events per pulse drops to 1/10th; the resulting cutoff frequencies are plotted in Fig. 5b. At 20nW incident power, repeated pulses are not detectable above 40Hz compared to a cutoff of 4kHz at 9631 nW. Above the cutoff, individual pulses are not detectable and instead, the laser appears as a continuous wave (CW) source: the laser is observable only when it is turned on or off. For a quasi-CW source pulsing at 1 Hz, we measured a detection limit of 1 ,2nW of time averaged incident power. \n\nIn another embodiment an optical dispersive element can be selected and used to determine not only the incident angle of the incoming laser, but also the wavelength of the laser. Having this additional information can help to connect the recorded information about direction and wavelength to specifically identify the laser that is detected by the LWS. \nThe optical dispersive element is configured to cause the incoming laser to be split in a diffraction pattern across the sensor of the camera. This diffraction pattern can be provided by positioning a modified transparent or reflective surface configured to create the diffraction pattern across the camera sensor along the path travelled by the incoming laser to the camera sensor. In one embodiment, this effect can be achieved by providing a phase grating along the optical dispersive element where the optical thickness varies along one axis of the optical dispersive element. Alternatively, a patterned series of opaque and transparent sections closely placed together (an amplitude grating) to create series of lines or gaps which can produce a ‘double-slit’ like pattern on the camera sensor when a laser passes through the pattern. The pattern, or grating profile, may be provided in: a grating, a 2D grid, combinations thereof or other such patterns known to one skilled in the art. The dispersive element may comprise: a separate element positioned in advance of, inside of, or after the lens, a film applied to the lens; by directly etching or placing the pattern directly onto the lens face; a dispersive prism; or by other means known to one skilled in the art. In a preferred embodiment the optical dispersive element is selected from: a holographic film, a holographic grating on a rigid substrate, an etched glass relief grating, or a polymer replicated grating. Functionally, any optical element capable of spatially separating the incoming laser in accordance with the wavelength of the incoming laser, such as those used in spectrometers, would be a possible optical dispersive element. The spacing of the pattern or grating profile along the dispersive element could have a density of between 100 sections/mm to 1500 section/mm, preferably a density of 1000 sections/mm may be used. \n\nAs shown in Figure 6 a laser passing through the optical dispersive element will generate a diffraction pattern with a series of peaks on the sensor of the camera. Based on the selection of the density of the pattern and the incidence angle of the incoming laser, there will be a consistent relationship between the wavelength of the incoming laser and the spacing between the peaks of the diffraction pattern. The diffraction element adds a first order signal that has a modified incoming angle. The difference of incident angle allows for the calculation of the wavelength. This difference in incidence angle is represented in the gap between the peaks of the diffraction pattern on the camera sensor. The processor receiving the image from the camera sensor can implement an algorithm to measure the spacing between the peaks and convert that value into the wavelength of the incoming laser. \nAs further shown in Figure 6 the diffraction pattern will generate different positions and number of peaks on the sensor based on the incidence angle of the incoming laser. The optical dispersive element is selected to have a grating profile which will generate between 2-5 peaks on the camera sensor dependent upon the incidence angle of the incoming laser. Some acceptable profiles include sinusoidal, square wave, triangle wave, and sawtooth. In a preferred embodiment the optical dispersive element will create a pattern with a number of peaks on the sensor which is either two or three. Of these peaks, a 0th order peak must be identified to use the position of the 0th order peak on the sensor to determine the incidence angle of the incoming laser. Where three peaks are present the central peak will always be the 0th order peak. Where only two peaks are present the peak further from the center of the sensor will be the 0th order peak. In each of these arrangements the number of peaks generated by the optical dispersive element will be dependent on the incidence angle of the incoming laser. As such, where more than two or three peaks are present on the sensor the number of peaks can be used together with the known ranges in which a given number of peaks will be generated in order to assist in identifying which of the peaks is the 0th order peak. The processor receiving the image from the camera sensor can implement an algorithm to measure this and determine which peak is the 0th order peak in order to calculate the incidence angle of the incoming laser in a fashion similar to that discussed above. \n\nHerein it is shown that a neuromorphic LWS has the potential to combine the advantages of photodiode-based LWSs and camera-based ones: high resolution, high sensitivity laser threat detection with a low power draw. \nThe strong frequency dependent response shown is a reminder that the ability of neuromorphic cameras to capture fast dynamics is not completely captured by the sensor's timing accuracy or latency metrics. The actual single pixel frequency response is significantly slower than the timing accuracy may suggest. \nUnless the context clearly requires otherwise, throughout the description and the claims, the words “comprise,” “comprising,” and the like are to be construed in an inclusive sense, as opposed to an exclusive or exhaustive sense; that is to say, in the sense of “including, but not limited to.” As used herein, the terms “connected,” “coupled,” or any variant thereof, means any connection or coupling, either direct or indirect, between two or more elements; the coupling of connection between the elements can be physical, logical, or a combination thereof. Additionally, the words “herein,” “above,” “below,” and words of similar import, when used in this application, shall refer to this application as a whole and not to any particular portions of this application. Where the context permits, words in the above Detailed Description using the singular or plural number may also include the plural or singular number respectively. The word “or,” in reference to a list of two or more items, covers all of the following interpretations of the word: any of the items in the list, all of the items in the list, and any combination of the items in the list.\nThe above detailed description of implementations of the system is not intended to be exhaustive or to limit the system to the precise form disclosed above. While specific implementations of, and examples for, the system are described above for illustrative purposes, various equivalent modifications are possible within the scope of the system, as those skilled in the relevant art will recognize. In addition, while processes, message/data flows, or blocks are presented in a given order, alternative implementations may perform routines having blocks, or employ systems having blocks, in a different order, and some processes or blocks may be deleted, moved, added, \n\nsubdivided, combined, and/or modified to provide alternative or sub-combinations. Each of these processes, message/data flows, or blocks may be implemented in a variety of different ways. Also, while processes or blocks are at times shown as being performed in series, these processes or blocks may instead be performed in parallel, or may be performed at different times. Further, any specific numbers noted herein are only examples: alternative implementations may employ differing values or ranges. Those skilled in the art will also appreciate that the actual implementation of a database may take a variety of forms, and the term “database” is used herein in the generic sense to refer to any data structure that allows data to be stored and accessed, such as tables, linked lists, arrays, etc. \nThe teachings of the methods and system provided herein can be applied to other systems, not necessarily the system described above. The elements, blocks and acts of the various implementations described above can be combined to provide further implementations. \nAny patents and applications and other references noted above, including any that may be listed in accompanying filing papers, are incorporated herein by reference. Aspects of the technology can be modified, if necessary, to employ the systems, functions, and concepts of the various references described above to provide yet further implementations of the technology. \nThese and other changes can be made to the invention in light of the above Detailed Description. While the above description describes certain implementations of the technology, and describes the best mode contemplated, no matter how detailed the above appears in text, the invention can be practiced in many ways. Details of the system may vary considerably in its implementation details, while still being encompassed by the technology disclosed herein. As noted above, particular terminology used when describing certain features or aspects of the technology should not be taken to imply that the terminology is being redefined herein to be restricted to any specific characteristics, features, or aspects of the technology with which that terminology is associated. In general, the terms used in the following claims should not \n\nbe construed to limit the invention to the specific implementations disclosed in the specification, unless the above Detailed Description section explicitly defines such terms. Accordingly, the actual scope of the invention encompasses not only the disclosed implementations, but also all equivalent ways of practicing or implementing the invention under the claims. \nWhile certain aspects of the technology are presented below in certain claim forms, the inventors contemplate the various aspects of the technology in any number of claim forms. For example, while only one aspect of the invention is recited as implemented in a computer-readable medium, other aspects may likewise be implemented in a computer-readable medium. Accordingly, the inventors reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the technology.",
  "prior_art_keywords": [
    "laser"
  ],
  "legal_events": [
    {
      "date": "2015-01-14",
      "event": "2016-07-14"
    },
    {
      "date": "2012-07-12",
      "event": "2016-07-21"
    },
    {
      "date": "2019-03-22",
      "event": "2021-04-01"
    },
    {
      "date": "2021-03-15",
      "event": "2022-09-15"
    },
    {
      "date": "2012-07-12",
      "event": "2016-07-21"
    },
    {
      "date": "2015-01-14",
      "event": "2016-07-14"
    },
    {
      "date": "2019-03-22",
      "event": "2021-04-01"
    },
    {
      "date": "2021-03-15",
      "event": "2022-09-15"
    },
    {
      "date": "2024-03-05",
      "event": "Limitation of noise on light detectors using an aperture"
    },
    {
      "date": "1994-03-15",
      "event": "Wave front sensor"
    },
    {
      "date": "1992-10-27",
      "event": "Optical measurement device with enhanced sensitivity"
    },
    {
      "date": "1997-04-15",
      "event": "Image multispectral sensing"
    },
    {
      "date": "1995-04-25",
      "event": "Method and apparatus for holographic wavefront diagnostics"
    },
    {
      "date": "2014-07-17",
      "event": "Optical monitoring device for an imaging system"
    },
    {
      "date": "2002-11-05",
      "event": "Light scattering apparatus and method for determining radiation exposure to plastic detectors"
    },
    {
      "date": "1998-11-11",
      "event": "Laser beam angle detection"
    },
    {
      "date": "2003-05-21",
      "event": "Level measuring device"
    },
    {
      "date": "2003-04-09",
      "event": "Measuring device and measuring method for minute particle group"
    },
    {
      "date": "2002-08-13",
      "event": "Sun optical limitation illumination detector (SOLID)"
    },
    {
      "date": "2016-09-13",
      "event": "Method and apparatus for measuring the shape of a wave-front of an optical radiation field"
    },
    {
      "date": "2024-10-24",
      "event": "Optical dispersive element for use with neuromorphic camera in a laser warning systems (lws)"
    },
    {
      "date": "2018-03-15",
      "event": "Method and device for operating a spectrometer"
    },
    {
      "date": "2025-04-30",
      "event": "Neuromorphic camera in a laser warning systems (lws)"
    },
    {
      "date": "1976-02-10",
      "event": "All-sky photoelectric lightning detector apparatus"
    },
    {
      "date": "2008-07-31",
      "event": "Two-detector gas filter correlation radiometry (GFCR) system using two-dimensional array detection of defocused image and detected-signal summation"
    },
    {
      "date": "2019-04-23",
      "event": "Scanning multispectral telescope comprising wavefront analysis means"
    },
    {
      "date": "2008-08-19",
      "event": "Low irradiance sensor with iterative angular resolution"
    },
    {
      "date": "1976-02-24",
      "event": "Photo-electric lightning detector apparatus"
    },
    {
      "date": "2021-11-16",
      "event": "Device for imaging and delivering spectroscopic information"
    }
  ],
  "cited_patents": null,
  "citing_patents": null,
  "family_members": null,
  "classification_codes": null,
  "raw_html_length": 145980
}